#!/usr/bin/env python3
"""
åŠ å¯†æ”¿ç­– RSS -> ä¼ä¸šå¾®ä¿¡æ¨é€å·¥å…·
æ”¯æŒå¤šæºèšåˆã€å…³é”®è¯è¿‡æ»¤ã€ä¸­æ–‡ç¿»è¯‘ã€è‡ªåŠ¨å»é‡
"""

from __future__ import annotations

import json
import logging
import os
import re
import sys
import time
import hashlib
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from functools import wraps
from typing import Any, Callable, Optional
from pathlib import Path

import feedparser
import requests
from dateutil import parser as date_parser
import translators as ts
import trafilatura
from openai import OpenAI

# ============== é…ç½® ==============

BASE_DIR = Path(__file__).parent
FEEDS_FILE = BASE_DIR / "feeds.json"
CONFIG_FILE = BASE_DIR / "config.json"
STATE_FILE = BASE_DIR / "state.json"

# ç¯å¢ƒå˜é‡
WECOM_WEBHOOK_URL = os.getenv("WECOM_WEBHOOK_URL", "")  # ä¼ä¸šå¾®ä¿¡ Webhook
DRY_RUN = os.getenv("DRY_RUN", "").lower() in ("1", "true", "yes")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()
DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY", "")  # DeepSeek API å¯†é’¥

# ============== æ—¥å¿—é…ç½® ==============

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# ============== æ•°æ®ç±» ==============

@dataclass
class FeedSource:
    """RSS æºé…ç½®"""
    name: str
    full_name: str
    url: str
    tags: list[str] = field(default_factory=list)
    enabled: bool = True


@dataclass
class FeedEntry:
    """RSS æ¡ç›®"""
    id: str
    title: str
    title_zh: str
    link: str
    summary: str
    summary_zh: str
    published: datetime
    source: str
    source_full: str
    tags: list[str] = field(default_factory=list)
    popularity_score: float = 0.0
    smart_summary: Optional[dict] = None  # LLMç”Ÿæˆçš„ç»“æ„åŒ–æ‘˜è¦


@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®"""
    keywords_allow: list[str] = field(default_factory=list)
    keywords_deny: list[str] = field(default_factory=list)
    http_timeout: int = 30
    max_entries_per_feed: int = 50
    state_retention_days: int = 30
    max_retries: int = 3
    retry_backoff_base: int = 2
    message_batch_size: int = 5
    message_delay: float = 1.0
    summary_max_length: int = 200
    tags_filter_enabled: bool = False
    tags_include: list[str] = field(default_factory=list)
    tags_exclude: list[str] = field(default_factory=list)
    # æ™ºèƒ½æ‘˜è¦é…ç½®
    smart_summary_enabled: bool = True
    smart_summary_score_threshold: int = 70
    smart_summary_max_content_length: int = 4000


# ============== ç®€æ˜“ä¸­è‹±ç¿»è¯‘ ==============

# å¸¸è§é‡‘è/åŠ å¯†æœ¯è¯­ä¸­è‹±å¯¹ç…§
TRANSLATION_DICT = {
    # æœºæ„å
    "Bank for International Settlements": "å›½é™…æ¸…ç®—é“¶è¡Œ",
    "International Monetary Fund": "å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡",
    "Federal Reserve": "ç¾è”å‚¨",
    "European Central Bank": "æ¬§æ´²å¤®è¡Œ",
    "Securities and Exchange Commission": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š",
    "Financial Conduct Authority": "è‹±å›½é‡‘èè¡Œä¸ºç›‘ç®¡å±€",
    "Monetary Authority of Singapore": "æ–°åŠ å¡é‡‘èç®¡ç†å±€",
    "Hong Kong Monetary Authority": "é¦™æ¸¯é‡‘èç®¡ç†å±€",
    "Bank of England": "è‹±æ ¼å…°é“¶è¡Œ",
    "Financial Stability Board": "é‡‘èç¨³å®šå§”å‘˜ä¼š",
    "People's Bank of China": "ä¸­å›½äººæ°‘é“¶è¡Œ",
    # æœ¯è¯­
    "cryptocurrency": "åŠ å¯†è´§å¸",
    "crypto": "åŠ å¯†",
    "bitcoin": "æ¯”ç‰¹å¸",
    "ethereum": "ä»¥å¤ªåŠ",
    "stablecoin": "ç¨³å®šå¸",
    "central bank digital currency": "å¤®è¡Œæ•°å­—è´§å¸",
    "CBDC": "å¤®è¡Œæ•°å­—è´§å¸",
    "digital currency": "æ•°å­—è´§å¸",
    "digital asset": "æ•°å­—èµ„äº§",
    "blockchain": "åŒºå—é“¾",
    "decentralized finance": "å»ä¸­å¿ƒåŒ–é‡‘è",
    "DeFi": "å»ä¸­å¿ƒåŒ–é‡‘è",
    "token": "ä»£å¸",
    "virtual asset": "è™šæ‹Ÿèµ„äº§",
    "virtual currency": "è™šæ‹Ÿè´§å¸",
    "NFT": "éåŒè´¨åŒ–ä»£å¸",
    "mining": "æŒ–çŸ¿",
    "exchange": "äº¤æ˜“æ‰€",
    "wallet": "é’±åŒ…",
    "custody": "æ‰˜ç®¡",
    "anti-money laundering": "åæ´—é’±",
    "AML": "åæ´—é’±",
    "know your customer": "äº†è§£ä½ çš„å®¢æˆ·",
    "KYC": "äº†è§£ä½ çš„å®¢æˆ·",
    "fintech": "é‡‘èç§‘æŠ€",
    "payment": "æ”¯ä»˜",
    "settlement": "ç»“ç®—",
    "clearing": "æ¸…ç®—",
    "regulation": "ç›‘ç®¡",
    "compliance": "åˆè§„",
    "enforcement": "æ‰§æ³•",
    "sanctions": "åˆ¶è£",
    "risk": "é£é™©",
    "financial stability": "é‡‘èç¨³å®š",
    "monetary policy": "è´§å¸æ”¿ç­–",
    "interest rate": "åˆ©ç‡",
    "inflation": "é€šèƒ€",
    "liquidity": "æµåŠ¨æ€§",
    "capital": "èµ„æœ¬",
    "asset": "èµ„äº§",
    "securities": "è¯åˆ¸",
    "derivatives": "è¡ç”Ÿå“",
    "futures": "æœŸè´§",
    "options": "æœŸæƒ",
    "trading": "äº¤æ˜“",
    "market": "å¸‚åœº",
    "investor": "æŠ•èµ„è€…",
    "consumer protection": "æ¶ˆè´¹è€…ä¿æŠ¤",
    "disclosure": "æŠ«éœ²",
    "transparency": "é€æ˜åº¦",
    "framework": "æ¡†æ¶",
    "guidance": "æŒ‡å¼•",
    "consultation": "å’¨è¯¢",
    "proposal": "ææ¡ˆ",
    "rule": "è§„åˆ™",
    "press release": "æ–°é—»ç¨¿",
    "statement": "å£°æ˜",
    "speech": "è®²è¯",
    "report": "æŠ¥å‘Š",
    "research": "ç ”ç©¶",
    "analysis": "åˆ†æ",
    "review": "å®¡æŸ¥",
    "assessment": "è¯„ä¼°",
}

# é¢„æ’åºç¿»è¯‘è¯å…¸ï¼ˆæŒ‰é•¿åº¦é™åºï¼Œä¼˜å…ˆåŒ¹é…é•¿è¯ç»„ï¼‰
SORTED_TRANSLATION_TERMS = sorted(TRANSLATION_DICT.keys(), key=len, reverse=True)


def translate_text(text: str) -> str:
    """
    ç®€æ˜“ç¿»è¯‘ï¼šåŸºäºè¯å…¸æ›¿æ¢å¸¸è§æœ¯è¯­
    å¯¹äºå®Œæ•´ç¿»è¯‘ï¼Œå»ºè®®æ¥å…¥ç¿»è¯‘ APIï¼ˆå¦‚ Google Translateã€DeepLï¼‰
    """
    if not text:
        return ""

    result = text
    # ä½¿ç”¨é¢„æ’åºåˆ—è¡¨ï¼ˆæ¨¡å—åŠ è½½æ—¶å·²æ’åºï¼‰
    for en_term in SORTED_TRANSLATION_TERMS:
        zh_term = TRANSLATION_DICT[en_term]
        # ä¸åŒºåˆ†å¤§å°å†™æ›¿æ¢
        pattern = re.compile(re.escape(en_term), re.IGNORECASE)
        result = pattern.sub(f"{zh_term}({en_term})", result)

    return result


def translate_to_chinese(text: str) -> str:
    """
    å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡
    ä½¿ç”¨ translators åº“ï¼ˆæ”¯æŒå¤šä¸ªç¿»è¯‘å¼•æ“è‡ªåŠ¨åˆ‡æ¢ï¼‰
    """
    if not text or not text.strip():
        return ""

    # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¿»è¯‘ï¼šåªæœ‰è‹±æ–‡å­—æ¯å æ¯”è¶³å¤Ÿé«˜æ‰ç¿»è¯‘
    # ç»Ÿè®¡è‹±æ–‡å­—æ¯æ•°é‡ï¼ˆä¸å«æ•°å­—ã€ç©ºæ ¼ã€æ ‡ç‚¹ï¼‰
    english_chars = len(re.findall(r'[a-zA-Z]', text))
    total_chars = len(text.strip())
    if total_chars == 0:
        return text
    # è‹±æ–‡å­—æ¯å æ¯”ä½äº40%ï¼Œè¯´æ˜å·²æ˜¯ä¸­æ–‡ä¸ºä¸»ï¼Œä¸ç¿»è¯‘
    if english_chars / total_chars < 0.4:
        return text

    # æˆªæ–­è¿‡é•¿æ–‡æœ¬
    if len(text) > 500:
        text = text[:500]

    # å°è¯•å¤šä¸ªç¿»è¯‘å¼•æ“
    engines = ['bing', 'google', 'alibaba', 'baidu']

    for engine in engines:
        try:
            translated = ts.translate_text(
                text,
                translator=engine,
                from_language='en',
                to_language='zh'
            )
            if translated and translated != text:
                return translated
        except Exception as e:
            logger.debug(f"{engine} ç¿»è¯‘å¤±è´¥: {e}")
            continue

    # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œä½¿ç”¨è¯å…¸ç¿»è¯‘
    logger.warning(f"æ‰€æœ‰ç¿»è¯‘å¼•æ“å¤±è´¥ï¼Œä½¿ç”¨è¯å…¸ç¿»è¯‘")
    return translate_text(text)


def translate_with_api(text: str, target_lang: str = "zh") -> str:
    """
    ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    """
    return translate_to_chinese(text)


# ============== é‡è¯•è£…é¥°å™¨ ==============

def retry_with_backoff(
    max_attempts: int = 3,
    backoff_base: int = 2,
    exceptions: tuple = (requests.RequestException,),
) -> Callable:
    """æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            last_exception = None
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts:
                        wait_time = backoff_base ** attempt
                        logger.warning(
                            f"{func.__name__} å¤±è´¥ (å°è¯• {attempt}/{max_attempts}): {e}"
                            f"ï¼Œ{wait_time}ç§’åé‡è¯•..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(
                            f"{func.__name__} æœ€ç»ˆå¤±è´¥ (å°è¯• {attempt}/{max_attempts}): {e}"
                        )
            raise last_exception
        return wrapper
    return decorator


# ============== é…ç½®åŠ è½½ ==============

def load_feeds() -> list[FeedSource]:
    """åŠ è½½ RSS æºé…ç½®"""
    if not FEEDS_FILE.exists():
        logger.error(f"RSS æºé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {FEEDS_FILE}")
        return []

    try:
        with open(FEEDS_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
    except json.JSONDecodeError as e:
        logger.error(f"RSS æºé…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
        return []
    except Exception as e:
        logger.error(f"è¯»å– RSS æºé…ç½®æ–‡ä»¶å¤±è´¥: {e}")
        return []

    feeds = []
    for item in data.get("feeds", []):
        if item.get("enabled", True):
            feeds.append(FeedSource(
                name=item["name"],
                full_name=item.get("full_name", item["name"]),
                url=item["url"],
                tags=item.get("tags", []),
                enabled=item.get("enabled", True),
            ))

    logger.info(f"å·²åŠ è½½ {len(feeds)} ä¸ª RSS æº")
    return feeds


def load_config() -> AppConfig:
    """åŠ è½½åº”ç”¨é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–"""
    config = AppConfig()

    if CONFIG_FILE.exists():
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            data = {}
        except Exception as e:
            logger.error(f"è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            data = {}
    else:
        data = {}

        keywords = data.get("keywords", {})
        config.keywords_allow = keywords.get("allow", [])
        config.keywords_deny = keywords.get("deny", [])

        settings = data.get("settings", {})
        config.http_timeout = settings.get("http_timeout_seconds", 30)
        config.max_entries_per_feed = settings.get("max_entries_per_feed", 50)
        config.state_retention_days = settings.get("state_retention_days", 30)
        config.max_retries = settings.get("max_retries", 3)
        config.retry_backoff_base = settings.get("retry_backoff_base", 2)
        config.message_batch_size = settings.get("message_batch_size", 5)
        config.message_delay = settings.get("message_delay_seconds", 1.0)
        config.summary_max_length = settings.get("summary_max_length", 200)

        tags_filter = data.get("tags_filter", {})
        config.tags_filter_enabled = tags_filter.get("enabled", False)
        config.tags_include = tags_filter.get("include_tags", [])
        config.tags_exclude = tags_filter.get("exclude_tags", [])

        # æ™ºèƒ½æ‘˜è¦é…ç½®
        smart_summary = data.get("smart_summary", {})
        config.smart_summary_enabled = smart_summary.get("enabled", True)
        config.smart_summary_score_threshold = smart_summary.get("score_threshold", 70)
        config.smart_summary_max_content_length = smart_summary.get("max_content_length", 4000)

    # ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆå¸¦ç±»å‹è½¬æ¢é”™è¯¯å¤„ç†ï¼‰
    def safe_int_env(name: str, default: int) -> int:
        val = os.getenv(name)
        if val:
            try:
                return int(val)
            except ValueError:
                logger.warning(f"ç¯å¢ƒå˜é‡ {name}={val} ä¸æ˜¯æœ‰æ•ˆæ•´æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ {default}")
        return default

    config.http_timeout = safe_int_env("HTTP_TIMEOUT", config.http_timeout)
    config.max_entries_per_feed = safe_int_env("MAX_ENTRIES_PER_FEED", config.max_entries_per_feed)
    config.state_retention_days = safe_int_env("STATE_RETENTION_DAYS", config.state_retention_days)

    # æ™ºèƒ½æ‘˜è¦ç¯å¢ƒå˜é‡è¦†ç›–
    if os.getenv("SMART_SUMMARY_ENABLED"):
        config.smart_summary_enabled = os.getenv("SMART_SUMMARY_ENABLED", "").lower() in ("1", "true", "yes")

    logger.info(f"é…ç½®å·²åŠ è½½: å…è®¸å…³é”®è¯ {len(config.keywords_allow)} ä¸ªï¼Œ"
                f"æ‹’ç»å…³é”®è¯ {len(config.keywords_deny)} ä¸ª")
    return config


# ============== çŠ¶æ€ç®¡ç† ==============

def load_state() -> dict[str, Any]:
    """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
    if not STATE_FILE.exists():
        return {"sent_ids": {}, "last_run": None}

    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            state = json.load(f)
            # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
            if "sent_ids" not in state:
                state["sent_ids"] = {}
            return state
    except json.JSONDecodeError as e:
        logger.error(f"çŠ¶æ€æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}ï¼Œé‡æ–°åˆå§‹åŒ–")
        return {"sent_ids": {}, "last_run": None}
    except Exception as e:
        logger.error(f"è¯»å–çŠ¶æ€æ–‡ä»¶å¤±è´¥: {e}ï¼Œé‡æ–°åˆå§‹åŒ–")
        return {"sent_ids": {}, "last_run": None}


def save_state(state: dict[str, Any]) -> None:
    """ä¿å­˜çŠ¶æ€æ–‡ä»¶ï¼ˆåŸå­å†™å…¥ï¼Œé¿å…å†™å…¥ä¸­æ–­å¯¼è‡´æ–‡ä»¶æŸåï¼‰"""
    state["last_run"] = datetime.now(timezone.utc).isoformat()

    # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œå†åŸå­æ€§é‡å‘½å
    temp_file = STATE_FILE.with_suffix(".tmp")
    try:
        with open(temp_file, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
        # åŸå­æ€§é‡å‘½åï¼ˆåŒä¸€æ–‡ä»¶ç³»ç»Ÿå†…æ˜¯åŸå­æ“ä½œï¼‰
        temp_file.replace(STATE_FILE)
        logger.info("çŠ¶æ€å·²ä¿å­˜")
    except Exception as e:
        logger.error(f"ä¿å­˜çŠ¶æ€å¤±è´¥: {e}")
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file.exists():
            temp_file.unlink()
        raise


def cleanup_state(state: dict[str, Any], retention_days: int) -> dict[str, Any]:
    """æ¸…ç†è¿‡æœŸçš„çŠ¶æ€è®°å½•"""
    cutoff = datetime.now(timezone.utc) - timedelta(days=retention_days)
    cutoff_str = cutoff.isoformat()

    sent_ids = state.get("sent_ids", {})
    original_count = len(sent_ids)

    # ä¿ç•™æœªè¿‡æœŸçš„è®°å½•
    cleaned = {
        entry_id: timestamp
        for entry_id, timestamp in sent_ids.items()
        if timestamp > cutoff_str
    }

    removed_count = original_count - len(cleaned)
    if removed_count > 0:
        logger.info(f"å·²æ¸…ç† {removed_count} æ¡è¿‡æœŸçŠ¶æ€è®°å½•")

    state["sent_ids"] = cleaned
    return state


def generate_entry_id(entry: dict, source_name: str) -> str:
    """ç”Ÿæˆæ¡ç›®å”¯ä¸€ ID"""
    # ä¼˜å…ˆä½¿ç”¨ entry è‡ªå¸¦çš„ id
    if entry.get("id"):
        return f"{source_name}:{entry['id']}"

    # å¦åˆ™ä½¿ç”¨ link çš„ hash
    if entry.get("link"):
        link_hash = hashlib.md5(entry["link"].encode()).hexdigest()[:12]
        return f"{source_name}:{link_hash}"

    # æœ€åä½¿ç”¨æ ‡é¢˜çš„ hash
    title_hash = hashlib.md5(entry.get("title", "").encode()).hexdigest()[:12]
    return f"{source_name}:{title_hash}"


# ============== RSS æŠ“å– ==============

@retry_with_backoff(max_attempts=3, backoff_base=2)
def fetch_feed(url: str, timeout: int = 30) -> feedparser.FeedParserDict:
    """æŠ“å– RSS æº"""
    # ä½¿ç”¨ requests è·å–å†…å®¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶è¶…æ—¶å’Œé‡è¯•
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; CryptoPolicyBot/1.0)",
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
    }

    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()

    return feedparser.parse(response.content)


def parse_entry_date(entry: dict) -> datetime:
    """è§£ææ¡ç›®å‘å¸ƒæ—¶é—´ï¼Œç¡®ä¿è¿”å› timezone-aware datetime"""
    # å°è¯•å¤šä¸ªæ—¥æœŸå­—æ®µ
    for date_field in ["published", "updated", "created"]:
        if entry.get(date_field):
            try:
                dt = date_parser.parse(entry[date_field])
                # ç¡®ä¿æ—¶åŒºæ„ŸçŸ¥
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except (ValueError, TypeError):
                continue

    # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
    return datetime.now(timezone.utc)


def extract_summary(entry: dict, max_length: int = 200) -> str:
    """æå–æ‘˜è¦"""
    summary = ""

    # å°è¯•å¤šä¸ªæ‘˜è¦å­—æ®µ
    if entry.get("summary"):
        summary = entry["summary"]
    elif entry.get("description"):
        summary = entry["description"]
    elif entry.get("content"):
        contents = entry["content"]
        if isinstance(contents, list) and contents:
            summary = contents[0].get("value", "")

    # æ¸…ç† HTML æ ‡ç­¾
    summary = re.sub(r"<[^>]+>", "", summary)
    summary = re.sub(r"\s+", " ", summary).strip()

    # æˆªæ–­
    if len(summary) > max_length:
        summary = summary[:max_length].rsplit(" ", 1)[0] + "..."

    return summary


def matches_keywords(text: str, keywords: list[str]) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…å…³é”®è¯ï¼ˆä½¿ç”¨è¯è¾¹ç•Œé¿å…å­ä¸²è¯¯åŒ¹é…ï¼‰"""
    text_lower = text.lower()
    for kw in keywords:
        # ä½¿ç”¨è¯è¾¹ç•Œ \b é¿å…å­ä¸²è¯¯åŒ¹é…ï¼ˆå¦‚ "etf" ä¸ä¼šåŒ¹é… "platform"ï¼‰
        pattern = r'\b' + re.escape(kw.lower()) + r'\b'
        if re.search(pattern, text_lower):
            return True
    return False


# ============== ç½‘é¡µå†…å®¹æŠ“å– ==============

def fetch_article_content(url: str, max_length: int = 4000, timeout: int = 30) -> str:
    """
    æŠ“å–ç½‘é¡µæ­£æ–‡å†…å®¹

    Args:
        url: æ–‡ç« é“¾æ¥
        max_length: æœ€å¤§å†…å®¹é•¿åº¦
        timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

    Returns:
        æ­£æ–‡å†…å®¹ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
    """
    if not url:
        return ""

    try:
        # ä½¿ç”¨ trafilatura æŠ“å–å’Œæå–æ­£æ–‡
        # è®¾ç½®é…ç½®ä»¥æ§åˆ¶è¶…æ—¶
        config = trafilatura.settings.use_config()
        config.set("DEFAULT", "DOWNLOAD_TIMEOUT", str(timeout))
        downloaded = trafilatura.fetch_url(url, config=config)
        if not downloaded:
            logger.warning(f"æ— æ³•ä¸‹è½½é¡µé¢: {url}")
            return ""

        # æå–æ­£æ–‡
        content = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=False,
            no_fallback=False,
        )

        if not content:
            logger.warning(f"æ— æ³•æå–æ­£æ–‡: {url}")
            return ""

        # æˆªæ–­è¿‡é•¿å†…å®¹
        if len(content) > max_length:
            content = content[:max_length]
            # å°è¯•åœ¨å¥å­è¾¹ç•Œæˆªæ–­
            last_period = content.rfind(".")
            if last_period > max_length * 0.8:
                content = content[:last_period + 1]

        logger.debug(f"æˆåŠŸæŠ“å–æ­£æ–‡: {url} ({len(content)} å­—ç¬¦)")
        return content

    except Exception as e:
        logger.warning(f"æŠ“å–æ­£æ–‡å¤±è´¥ {url}: {e}")
        return ""


# ============== DeepSeek æ™ºèƒ½æ‘˜è¦ ==============

def generate_smart_summary(title: str, content: str) -> Optional[dict]:
    """
    ä½¿ç”¨ DeepSeek API ç”Ÿæˆç»“æ„åŒ–æ‘˜è¦

    Args:
        title: æ–‡ç« æ ‡é¢˜
        content: æ–‡ç« æ­£æ–‡

    Returns:
        ç»“æ„åŒ–æ‘˜è¦å­—å…¸ï¼Œå¤±è´¥è¿”å› None
        {
            "core_point": "ä¸€å¥è¯æ ¸å¿ƒï¼ˆ20å­—å†…ï¼‰",
            "key_data": ["å…³é”®æ•°æ®1", "å…³é”®æ•°æ®2"],
            "impact": "å½±å“è¯„ä¼°ï¼ˆ30å­—å†…ï¼‰"
        }
    """
    if not DEEPSEEK_API_KEY:
        logger.warning("DEEPSEEK_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æ™ºèƒ½æ‘˜è¦")
        return None

    if not content:
        logger.debug("æ­£æ–‡ä¸ºç©ºï¼Œè·³è¿‡æ™ºèƒ½æ‘˜è¦")
        return None

    try:
        client = OpenAI(
            api_key=DEEPSEEK_API_KEY,
            base_url="https://api.deepseek.com"
        )

        prompt = f"""åˆ†æä»¥ä¸‹åŠ å¯†è´§å¸/é‡‘èæ–°é—»ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚

æ ‡é¢˜ï¼š{title}

æ­£æ–‡ï¼š
{content[:3000]}

è¯·ç”¨ä¸­æ–‡å›å¤ï¼Œä¸¥æ ¼æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºï¼ˆä¸è¦æ·»åŠ å…¶ä»–å†…å®¹ï¼‰ï¼š
{{
    "core_point": "ä¸€å¥è¯æ¦‚æ‹¬æ ¸å¿ƒå†…å®¹ï¼ˆä¸è¶…è¿‡20å­—ï¼‰",
    "key_data": ["å…³é”®æ•°æ®ç‚¹1", "å…³é”®æ•°æ®ç‚¹2"],
    "impact": "å¯¹å¸‚åœº/è¡Œä¸šçš„å½±å“ï¼ˆä¸è¶…è¿‡30å­—ï¼‰"
}}

æ³¨æ„ï¼š
- core_point å¿…é¡»ç®€æ´æœ‰åŠ›ï¼ŒæŠ“ä½æ–°é—»æ ¸å¿ƒ
- key_data æå–å…·ä½“æ•°å­—ã€é‡‘é¢ã€æ—¶é—´ã€æ¯”ä¾‹ç­‰æ•°æ®ï¼Œæ²¡æœ‰åˆ™è¿”å›ç©ºæ•°ç»„
- impact è¯„ä¼°è¿™æ¡æ–°é—»çš„å®é™…å½±å“ï¼Œååˆ©å¥½/åˆ©ç©º/ä¸­æ€§"""

        response = client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸“ä¸šçš„åŠ å¯†è´§å¸æ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿æç‚¼å…³é”®ä¿¡æ¯ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_tokens=500,
        )

        result_text = response.choices[0].message.content.strip()

        # å°è¯•è§£æ JSON
        # å¤„ç†å¯èƒ½çš„ markdown ä»£ç å—
        if "```" in result_text:
            # æå–ä»£ç å—å†…å®¹
            parts = result_text.split("```")
            for part in parts[1::2]:  # å–å¥‡æ•°ç´¢å¼•ï¼ˆä»£ç å—å†…å®¹ï¼‰
                part = part.strip()
                if part.startswith("json"):
                    part = part[4:].strip()
                if part.startswith("{"):
                    result_text = part
                    break

        # å¦‚æœè¿˜ä¸æ˜¯ä»¥ { å¼€å¤´ï¼Œå°è¯•ç”¨æ­£åˆ™æå– JSON å¯¹è±¡
        if not result_text.strip().startswith("{"):
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', result_text, re.DOTALL)
            if json_match:
                result_text = json_match.group()

        summary = json.loads(result_text.strip())

        # éªŒè¯å¿…è¦å­—æ®µ
        if "core_point" not in summary:
            logger.warning("æ™ºèƒ½æ‘˜è¦ç¼ºå°‘ core_point å­—æ®µ")
            return None

        # ç¡®ä¿ key_data æ˜¯åˆ—è¡¨
        if "key_data" not in summary:
            summary["key_data"] = []
        elif not isinstance(summary["key_data"], list):
            summary["key_data"] = [summary["key_data"]]

        # ç¡®ä¿ impact å­˜åœ¨
        if "impact" not in summary:
            summary["impact"] = ""

        logger.debug(f"æ™ºèƒ½æ‘˜è¦ç”ŸæˆæˆåŠŸ: {summary['core_point']}")
        return summary

    except json.JSONDecodeError as e:
        logger.warning(f"æ™ºèƒ½æ‘˜è¦ JSON è§£æå¤±è´¥: {e}")
        return None
    except Exception as e:
        logger.warning(f"æ™ºèƒ½æ‘˜è¦ç”Ÿæˆå¤±è´¥: {e}")
        return None


def filter_entry(
    entry: dict,
    config: AppConfig,
    source_tags: list[str],
) -> bool:
    """è¿‡æ»¤æ¡ç›®ï¼šè¿”å› True è¡¨ç¤ºä¿ç•™"""
    title = entry.get("title", "")
    summary = entry.get("summary", "") or entry.get("description", "")
    combined_text = f"{title} {summary}"

    # æ£€æŸ¥æ‹’ç»å…³é”®è¯
    if config.keywords_deny and matches_keywords(combined_text, config.keywords_deny):
        return False

    # æ£€æŸ¥å…è®¸å…³é”®è¯ï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
    if config.keywords_allow:
        if not matches_keywords(combined_text, config.keywords_allow):
            return False

    # æ£€æŸ¥æ ‡ç­¾è¿‡æ»¤
    if config.tags_filter_enabled:
        if config.tags_exclude:
            if any(tag in config.tags_exclude for tag in source_tags):
                return False
        if config.tags_include:
            if not any(tag in config.tags_include for tag in source_tags):
                return False

    return True


# ============== çƒ­åº¦è¯„åˆ†ç³»ç»Ÿ ==============

# æ¥æºæƒå¨æ€§æƒé‡ (0-35åˆ†)
SOURCE_WEIGHTS = {
    "SEC": 35,
    "CoinDesk": 30,
    "TheBlock": 28,
    "Cointelegraph": 26,
    "Decrypt": 22,
    "CryptoSlate": 20,
    "CardanoSpot": 18,
    "IOHK-Blog": 18,
    "AdaPulse": 16,
    "Cardano-Forum": 15,
    "U.Today": 14,
    "NewsBTC": 12,
    "BeInCrypto": 12,
}

# æ ‡é¢˜çƒ­è¯ (ç´¯è®¡æœ€é«˜25åˆ†)
HOT_KEYWORDS = {
    "breaking": 10,
    "surge": 5,
    "crash": 5,
    "etf": 6,
    "approved": 7,
    "sec": 5,
    "regulation": 4,
    "bitcoin": 3,
    "ethereum": 3,
    "ban": 5,
    "hack": 5,
    "lawsuit": 4,
    "settlement": 4,
    "partnership": 3,
    "launch": 3,
    "upgrade": 3,
    "mainnet": 4,
    "airdrop": 3,
}

# Cardano å…³é”®è¯ï¼ˆç”¨äºåŠ åˆ†ï¼‰
CARDANO_KEYWORDS = ["cardano", "ada", "iohk", "hoskinson", "plutus", "hydra", "midnight", "lace", "voltaire"]


def calculate_popularity_score(entry: FeedEntry) -> float:
    """
    è®¡ç®—æ–‡ç« çƒ­åº¦è¯„åˆ†

    è¯„åˆ†ç»´åº¦ï¼š
    1. æ¥æºæƒå¨æ€§ (0-35åˆ†)
    2. æ ‡é¢˜çƒ­è¯ (0-25åˆ†)
    3. æ—¶æ•ˆæ€§ (0-20åˆ†)
    4. å†…å®¹è´¨é‡å¯å‘ (0-15åˆ†)
    5. Cardano åŠ åˆ† (0-20åˆ†)

    æ€»åˆ†èŒƒå›´: 0-115åˆ†
    """
    score = 0.0

    # 1. æ¥æºæƒå¨æ€§ (0-35åˆ†)
    source_score = SOURCE_WEIGHTS.get(entry.source, 10)
    score += source_score

    # 2. æ ‡é¢˜çƒ­è¯ (0-25åˆ†ï¼Œç´¯è®¡)
    title_lower = entry.title.lower()
    keyword_score = 0
    for keyword, points in HOT_KEYWORDS.items():
        if keyword in title_lower:
            keyword_score += points
    score += min(keyword_score, 25)  # ä¸Šé™25åˆ†

    # 3. æ—¶æ•ˆæ€§ (0-20åˆ†)
    now = datetime.now(timezone.utc)
    age_hours = (now - entry.published).total_seconds() / 3600
    if age_hours <= 2:
        score += 20
    elif age_hours <= 6:
        score += 15
    elif age_hours <= 12:
        score += 10
    elif age_hours <= 24:
        score += 5

    # 4. å†…å®¹è´¨é‡å¯å‘ (0-15åˆ†)
    # æ ‡é¢˜é•¿åº¦é€‚ä¸­ (20-80å­—ç¬¦) +5åˆ†
    title_len = len(entry.title)
    if 20 <= title_len <= 80:
        score += 5
    # æœ‰æ‘˜è¦ +5åˆ†
    if entry.summary and len(entry.summary) > 50:
        score += 5
    # æœ‰é“¾æ¥ +5åˆ†
    if entry.link:
        score += 5

    # 5. Cardano åŠ åˆ† (0-30åˆ†ï¼Œç¡®ä¿ä¼˜å…ˆæ¨é€)
    combined_text = f"{entry.title} {entry.summary}".lower()
    if any(kw in combined_text for kw in CARDANO_KEYWORDS):
        score += 20  # å…³é”®è¯åŒ¹é…
    # æ¥è‡ª Cardano ä¸“ç”¨æº +10åˆ†
    if any(tag in ["cardano", "ada"] for tag in entry.tags):
        score += 10

    return score


def process_feed(
    source: FeedSource,
    config: AppConfig,
    sent_ids: set[str],
) -> list[FeedEntry]:
    """å¤„ç†å•ä¸ª RSS æº"""
    logger.info(f"æ­£åœ¨æŠ“å–: {source.name} ({source.url})")

    try:
        feed = fetch_feed(source.url, config.http_timeout)
    except Exception as e:
        logger.error(f"æŠ“å– {source.name} å¤±è´¥: {e}")
        return []

    entries = []
    for raw_entry in feed.entries[:config.max_entries_per_feed]:
        entry_id = generate_entry_id(raw_entry, source.name)

        # è·³è¿‡å·²å‘é€
        if entry_id in sent_ids:
            continue

        # è¿‡æ»¤
        if not filter_entry(raw_entry, config, source.tags):
            continue

        # æå–ä¿¡æ¯
        title = raw_entry.get("title", "æ— æ ‡é¢˜")
        summary = extract_summary(raw_entry, config.summary_max_length)

        # ç¿»è¯‘ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶ï¼‰
        title_zh = translate_to_chinese(title)
        time.sleep(1.0)  # é¿å…ç¿»è¯‘APIé€Ÿç‡é™åˆ¶
        summary_zh = translate_to_chinese(summary)
        time.sleep(1.0)

        entry = FeedEntry(
            id=entry_id,
            title=title,
            title_zh=title_zh,
            link=raw_entry.get("link", ""),
            summary=summary,
            summary_zh=summary_zh,
            published=parse_entry_date(raw_entry),
            source=source.name,
            source_full=source.full_name,
            tags=source.tags,
        )
        entries.append(entry)

    logger.info(f"{source.name}: å‘ç° {len(entries)} æ¡æ–°æ¡ç›®")
    return entries


# ============== ä¼ä¸šå¾®ä¿¡å‘é€ ==============

def get_importance_level(score: float) -> tuple[str, str]:
    """
    æ ¹æ®çƒ­åº¦è¯„åˆ†è·å–é‡è¦åº¦çº§åˆ«

    Returns:
        (emojiæ ‡ç­¾, çº§åˆ«åç§°)
    """
    if score >= 70:
        return "ğŸ”´", "å¿…è¯»"
    elif score >= 50:
        return "ğŸŸ¡", "é‡è¦"
    else:
        return "ğŸŸ¢", "å‚è€ƒ"


def format_wecom_markdown(entries: list[FeedEntry]) -> str:
    """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡ Markdown æ¶ˆæ¯ï¼ˆä¸­æ–‡ç‰ˆï¼Œæ”¯æŒåˆ†çº§æ˜¾ç¤ºï¼‰"""
    # åŒ—äº¬æ—¶é—´
    beijing_tz = timezone(timedelta(hours=8))
    now_beijing = datetime.now(beijing_tz)

    lines = [
        "# ğŸ“š åŠ å¯†æ”¿ç­–/ç ”æŠ¥é€Ÿè§ˆ",
        f"> â° {now_beijing.strftime('%Y-%m-%d %H:%M')} åŒ—äº¬æ—¶é—´",
        "",
    ]

    for i, entry in enumerate(entries, 1):
        # è·å–é‡è¦åº¦çº§åˆ«
        emoji, level = get_importance_level(entry.popularity_score)

        # æ¥æºæ ‡ç­¾
        source_tag = f"[{entry.source}]"

        # æ ‡é¢˜è¡Œï¼šé‡è¦åº¦ + æ¥æº + æ ‡é¢˜
        lines.append(f"**{emoji} {level} | {source_tag} {entry.title_zh}**")

        # æ ¹æ®æ˜¯å¦æœ‰æ™ºèƒ½æ‘˜è¦å†³å®šæ˜¾ç¤ºæ ¼å¼
        if entry.smart_summary:
            # å¿…è¯»çº§åˆ«ï¼šæ˜¾ç¤ºç»“æ„åŒ–æ™ºèƒ½æ‘˜è¦
            summary = entry.smart_summary

            # æ ¸å¿ƒè§‚ç‚¹
            lines.append(f"ğŸ“Œ æ ¸å¿ƒï¼š{summary.get('core_point', '')}")

            # å…³é”®æ•°æ®
            key_data = summary.get('key_data', [])
            if key_data:
                data_str = " | ".join(key_data[:3])  # æœ€å¤š3ä¸ªæ•°æ®ç‚¹
                lines.append(f"ğŸ“Š æ•°æ®ï¼š{data_str}")

            # å½±å“è¯„ä¼°
            impact = summary.get('impact', '')
            if impact:
                lines.append(f"âš¡ å½±å“ï¼š{impact}")
        else:
            # æ™®é€šçº§åˆ«ï¼šæ˜¾ç¤ºRSSåŸæ‘˜è¦
            if entry.summary_zh:
                summary_text = entry.summary_zh[:150]
                if len(entry.summary_zh) > 150:
                    summary_text += "..."
                lines.append(f"> {summary_text}")

        # é“¾æ¥
        lines.append(f"[ğŸ‘‰ åŸæ–‡]({entry.link})")
        lines.append("")

    # æ ‡ç­¾
    all_tags = set()
    for entry in entries:
        all_tags.update(entry.tags)
    if all_tags:
        tag_str = " ".join(f"`#{tag}`" for tag in sorted(all_tags)[:5])
        lines.append(tag_str)

    return "\n".join(lines)


def format_wecom_text(entries: list[FeedEntry]) -> str:
    """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå¤‡ç”¨ï¼‰"""
    beijing_tz = timezone(timedelta(hours=8))
    now_beijing = datetime.now(beijing_tz)

    lines = [
        "ğŸ“š åŠ å¯†æ”¿ç­–/ç ”æŠ¥é€Ÿè§ˆ",
        f"â° {now_beijing.strftime('%Y-%m-%d %H:%M')} åŒ—äº¬æ—¶é—´",
        "â”" * 20,
        "",
    ]

    for i, entry in enumerate(entries, 1):
        lines.append(f"{i}. [{entry.source}] {entry.title_zh}")
        if entry.summary_zh:
            lines.append(f"   ğŸ“ {entry.summary_zh[:120]}...")
        lines.append(f"   ğŸ”— {entry.link}")
        lines.append("")

    return "\n".join(lines)


@retry_with_backoff(max_attempts=3, backoff_base=2)
def send_wecom_message(content: str, webhook_url: str, msg_type: str = "markdown") -> bool:
    """
    å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯

    Args:
        content: æ¶ˆæ¯å†…å®¹
        webhook_url: ä¼ä¸šå¾®ä¿¡ Webhook URL
        msg_type: æ¶ˆæ¯ç±»å‹ (markdown / text)
    """
    if msg_type == "markdown":
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "content": content
            }
        }
    else:
        payload = {
            "msgtype": "text",
            "text": {
                "content": content
            }
        }

    response = requests.post(webhook_url, json=payload, timeout=30)
    response.raise_for_status()

    result = response.json()
    if result.get("errcode") != 0:
        raise Exception(f"ä¼ä¸šå¾®ä¿¡ API é”™è¯¯: {result}")

    logger.debug(f"ä¼ä¸šå¾®ä¿¡å“åº”: {result}")
    return True


def send_entries(
    entries: list[FeedEntry],
    batch_size: int = 5,
    delay: float = 1.0,
) -> list[str]:
    """æ‰¹é‡å‘é€æ¡ç›®åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not entries:
        logger.info("æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å‘é€")
        return []

    if DRY_RUN:
        logger.info(f"[DRY-RUN] å°†å‘é€ {len(entries)} æ¡æ¶ˆæ¯")
        for entry in entries:
            logger.info(f"  - [{entry.source}] {entry.title_zh}")
        # æ‰“å°æ¶ˆæ¯é¢„è§ˆ
        preview = format_wecom_markdown(entries[:3])
        logger.info(f"\næ¶ˆæ¯é¢„è§ˆ:\n{preview}")
        return [e.id for e in entries]

    if not WECOM_WEBHOOK_URL:
        logger.error("ä¼ä¸šå¾®ä¿¡é…ç½®ç¼ºå¤±: è¯·è®¾ç½® WECOM_WEBHOOK_URL ç¯å¢ƒå˜é‡")
        return []

    sent_ids = []

    # åˆ†æ‰¹å‘é€ï¼ˆä¼ä¸šå¾®ä¿¡å•æ¡æ¶ˆæ¯é™åˆ¶ 4096 å­—èŠ‚ï¼‰
    MAX_MESSAGE_BYTES = 4000  # ç•™100å­—èŠ‚ä½™é‡

    for i in range(0, len(entries), batch_size):
        batch = entries[i:i + batch_size]
        message = format_wecom_markdown(batch)

        # å¦‚æœæ¶ˆæ¯è¿‡é•¿ï¼Œå°è¯•ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼
        if len(message.encode('utf-8')) > MAX_MESSAGE_BYTES:
            logger.warning("Markdownæ¶ˆæ¯è¿‡é•¿ï¼Œåˆ‡æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼")
            message = format_wecom_text(batch)

            # çº¯æ–‡æœ¬ä»è¶…é™ï¼Œé€æ¡æ‹†åˆ†å‘é€
            if len(message.encode('utf-8')) > MAX_MESSAGE_BYTES:
                logger.warning("çº¯æ–‡æœ¬ä»è¶…é™ï¼Œæ”¹ä¸ºé€æ¡å‘é€")
                # å°†å½“å‰æ‰¹æ¬¡æ‹†åˆ†ä¸ºå•æ¡å‘é€
                for single_entry in batch:
                    single_message = format_wecom_text([single_entry])
                    # å•æ¡ä»è¶…é™åˆ™æˆªæ–­
                    if len(single_message.encode('utf-8')) > MAX_MESSAGE_BYTES:
                        single_message = single_message[:MAX_MESSAGE_BYTES].rsplit('\n', 1)[0]
                    try:
                        send_wecom_message(single_message, WECOM_WEBHOOK_URL, "text")
                        sent_ids.append(single_entry.id)
                        time.sleep(delay)
                    except Exception as e:
                        logger.error(f"å•æ¡å‘é€å¤±è´¥ [{single_entry.source}] {single_entry.title[:30]}: {e}")
                continue  # è·³è¿‡åç»­æ‰¹æ¬¡å‘é€é€»è¾‘

        batch_num = i // batch_size + 1
        max_batch_retries = 3

        for retry in range(max_batch_retries):
            try:
                send_wecom_message(message, WECOM_WEBHOOK_URL, "markdown")
                sent_ids.extend(e.id for e in batch)
                logger.info(f"å·²å‘é€ç¬¬ {batch_num} æ‰¹ ({len(batch)} æ¡)")
                break  # å‘é€æˆåŠŸï¼Œè·³å‡ºé‡è¯•å¾ªç¯
            except Exception as e:
                if retry < max_batch_retries - 1:
                    wait_time = 2 ** (retry + 1)  # æŒ‡æ•°é€€é¿ï¼š2, 4, 8 ç§’
                    logger.warning(f"ç¬¬ {batch_num} æ‰¹å‘é€å¤±è´¥ (å°è¯• {retry + 1}/{max_batch_retries}): {e}ï¼Œ{wait_time}ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    # æœ€ç»ˆå¤±è´¥ï¼Œè®°å½•æœªå‘é€çš„æ¡ç›®
                    failed_titles = [e.title[:30] for e in batch]
                    logger.error(f"ç¬¬ {batch_num} æ‰¹æœ€ç»ˆå‘é€å¤±è´¥: {e}")
                    logger.error(f"æœªå‘é€æ¡ç›®: {failed_titles}")

        # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆä¼ä¸šå¾®ä¿¡é™åˆ¶æ¯åˆ†é’Ÿ 20 æ¡ï¼‰
        if i + batch_size < len(entries):
            time.sleep(delay)

    return sent_ids


# ============== ä¸»å‡½æ•° ==============

def main() -> int:
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("åŠ å¯†æ”¿ç­– RSS èšåˆå™¨å¯åŠ¨")
    logger.info(f"DRY_RUN: {DRY_RUN}")
    logger.info("=" * 50)

    # åŠ è½½é…ç½®
    feeds = load_feeds()
    if not feeds:
        logger.error("æ²¡æœ‰å¯ç”¨çš„ RSS æº")
        return 1

    config = load_config()

    # åŠ è½½å¹¶æ¸…ç†çŠ¶æ€
    state = load_state()
    state = cleanup_state(state, config.state_retention_days)
    sent_ids = set(state.get("sent_ids", {}).keys())

    # å¤„ç†æ‰€æœ‰ RSS æº
    all_entries: list[FeedEntry] = []

    for source in feeds:
        try:
            entries = process_feed(source, config, sent_ids)
            all_entries.extend(entries)
        except Exception as e:
            logger.error(f"å¤„ç† {source.name} æ—¶å‡ºé”™: {e}")
            # å•ä¸ªæºå¤±è´¥ä¸å½±å“å…¶ä»–æº
            continue

    # è®¡ç®—çƒ­åº¦è¯„åˆ†å¹¶æ’åº
    for entry in all_entries:
        entry.popularity_score = calculate_popularity_score(entry)

    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼ˆåŒåˆ†æŒ‰æ—¶é—´ï¼‰
    all_entries.sort(key=lambda e: (e.popularity_score, e.published), reverse=True)

    logger.info(f"å…±å‘ç° {len(all_entries)} æ¡æ–°æ¡ç›®")

    # åªæ¨é€å‰20æ¡æœ€é‡è¦çš„æ–‡ç« 
    MAX_DAILY_ENTRIES = 20
    if len(all_entries) > MAX_DAILY_ENTRIES:
        logger.info(f"ç­›é€‰å‰ {MAX_DAILY_ENTRIES} æ¡æœ€é‡è¦çš„æ–‡ç« ")
        all_entries = all_entries[:MAX_DAILY_ENTRIES]

    # æ‰“å°çƒ­åº¦è¯„åˆ†ï¼ˆè°ƒè¯•ç”¨ï¼‰
    if all_entries:
        logger.info("çƒ­åº¦æ’åå‰5:")
        for i, entry in enumerate(all_entries[:5], 1):
            emoji, level = get_importance_level(entry.popularity_score)
            logger.info(f"  {i}. {emoji}{level} [{entry.source}] {entry.title[:50]}... (è¯„åˆ†: {entry.popularity_score:.1f})")

    # ä¸ºé«˜åˆ†æ–‡ç« ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
    if config.smart_summary_enabled and DEEPSEEK_API_KEY:
        logger.info("å¼€å§‹ä¸ºå¿…è¯»çº§æ–‡ç« ç”Ÿæˆæ™ºèƒ½æ‘˜è¦...")
        smart_summary_count = 0

        for entry in all_entries:
            if entry.popularity_score >= config.smart_summary_score_threshold:
                logger.info(f"  æŠ“å–æ­£æ–‡: {entry.title[:40]}...")

                # æŠ“å–æ–‡ç« å…¨æ–‡
                content = fetch_article_content(
                    entry.link,
                    max_length=config.smart_summary_max_content_length,
                    timeout=config.http_timeout
                )

                if content:
                    # ç”Ÿæˆæ™ºèƒ½æ‘˜è¦
                    logger.info(f"  ç”Ÿæˆæ‘˜è¦: {entry.title[:40]}...")
                    summary = generate_smart_summary(entry.title, content)

                    if summary:
                        entry.smart_summary = summary
                        smart_summary_count += 1
                        logger.info(f"  âœ“ æ‘˜è¦å®Œæˆ: {summary.get('core_point', '')[:30]}...")
                    else:
                        logger.warning(f"  âœ— æ‘˜è¦ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨RSSåŸæ‘˜è¦")
                else:
                    logger.warning(f"  âœ— æ­£æ–‡æŠ“å–å¤±è´¥ï¼Œä½¿ç”¨RSSåŸæ‘˜è¦")

                # APIè°ƒç”¨é—´éš”
                time.sleep(1.0)

        logger.info(f"æ™ºèƒ½æ‘˜è¦å®Œæˆ: {smart_summary_count}/{len([e for e in all_entries if e.popularity_score >= config.smart_summary_score_threshold])} ç¯‡")
    elif config.smart_summary_enabled and not DEEPSEEK_API_KEY:
        logger.warning("æ™ºèƒ½æ‘˜è¦å·²å¯ç”¨ä½† DEEPSEEK_API_KEY æœªè®¾ç½®ï¼Œè·³è¿‡æ™ºèƒ½æ‘˜è¦")

    # å‘é€
    newly_sent = send_entries(
        all_entries,
        batch_size=config.message_batch_size,
        delay=config.message_delay,
    )

    # æ›´æ–°çŠ¶æ€
    now_iso = datetime.now(timezone.utc).isoformat()
    for entry_id in newly_sent:
        state["sent_ids"][entry_id] = now_iso

    save_state(state)

    logger.info(f"æœ¬æ¬¡è¿è¡Œå®Œæˆ: å‘é€ {len(newly_sent)} æ¡")
    logger.info("=" * 50)

    return 0


if __name__ == "__main__":
    sys.exit(main())
