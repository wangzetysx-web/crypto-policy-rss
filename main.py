#!/usr/bin/env python3
"""
åŠ å¯†æ”¿ç­– RSS -> ä¼ä¸šå¾®ä¿¡æ¨é€å·¥å…·
æ”¯æŒå¤šæºèšåˆã€å…³é”®è¯è¿‡æ»¤ã€ä¸­æ–‡ç¿»è¯‘ã€è‡ªåŠ¨å»é‡
"""

from __future__ import annotations

import json
import logging
import os
import re
import sys
import time
import hashlib
from dataclasses import dataclass, field, asdict
from datetime import datetime, timezone, timedelta
from functools import wraps
from typing import Any, Callable, Optional
from pathlib import Path

import feedparser
import requests
from dateutil import parser as date_parser
import translators as ts

# ============== é…ç½® ==============

BASE_DIR = Path(__file__).parent
FEEDS_FILE = BASE_DIR / "feeds.json"
CONFIG_FILE = BASE_DIR / "config.json"
STATE_FILE = BASE_DIR / "state.json"

# ç¯å¢ƒå˜é‡
WECOM_WEBHOOK_URL = os.getenv("WECOM_WEBHOOK_URL", "")  # ä¼ä¸šå¾®ä¿¡ Webhook
DRY_RUN = os.getenv("DRY_RUN", "").lower() in ("1", "true", "yes")
LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO").upper()

# ============== æ—¥å¿—é…ç½® ==============

logging.basicConfig(
    level=getattr(logging, LOG_LEVEL, logging.INFO),
    format="%(asctime)s | %(levelname)-8s | %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)
logger = logging.getLogger(__name__)

# ============== æ•°æ®ç±» ==============

@dataclass
class FeedSource:
    """RSS æºé…ç½®"""
    name: str
    full_name: str
    url: str
    tags: list[str] = field(default_factory=list)
    enabled: bool = True


@dataclass
class FeedEntry:
    """RSS æ¡ç›®"""
    id: str
    title: str
    title_zh: str
    link: str
    summary: str
    summary_zh: str
    published: datetime
    source: str
    source_full: str
    tags: list[str] = field(default_factory=list)


@dataclass
class AppConfig:
    """åº”ç”¨é…ç½®"""
    keywords_allow: list[str] = field(default_factory=list)
    keywords_deny: list[str] = field(default_factory=list)
    http_timeout: int = 30
    max_entries_per_feed: int = 50
    state_retention_days: int = 30
    max_retries: int = 3
    retry_backoff_base: int = 2
    message_batch_size: int = 5
    message_delay: float = 1.0
    summary_max_length: int = 200
    tags_filter_enabled: bool = False
    tags_include: list[str] = field(default_factory=list)
    tags_exclude: list[str] = field(default_factory=list)


# ============== ç®€æ˜“ä¸­è‹±ç¿»è¯‘ ==============

# å¸¸è§é‡‘è/åŠ å¯†æœ¯è¯­ä¸­è‹±å¯¹ç…§
TRANSLATION_DICT = {
    # æœºæ„å
    "Bank for International Settlements": "å›½é™…æ¸…ç®—é“¶è¡Œ",
    "International Monetary Fund": "å›½é™…è´§å¸åŸºé‡‘ç»„ç»‡",
    "Federal Reserve": "ç¾è”å‚¨",
    "European Central Bank": "æ¬§æ´²å¤®è¡Œ",
    "Securities and Exchange Commission": "ç¾å›½è¯åˆ¸äº¤æ˜“å§”å‘˜ä¼š",
    "Financial Conduct Authority": "è‹±å›½é‡‘èè¡Œä¸ºç›‘ç®¡å±€",
    "Monetary Authority of Singapore": "æ–°åŠ å¡é‡‘èç®¡ç†å±€",
    "Hong Kong Monetary Authority": "é¦™æ¸¯é‡‘èç®¡ç†å±€",
    "Bank of England": "è‹±æ ¼å…°é“¶è¡Œ",
    "Financial Stability Board": "é‡‘èç¨³å®šå§”å‘˜ä¼š",
    "People's Bank of China": "ä¸­å›½äººæ°‘é“¶è¡Œ",
    # æœ¯è¯­
    "cryptocurrency": "åŠ å¯†è´§å¸",
    "crypto": "åŠ å¯†",
    "bitcoin": "æ¯”ç‰¹å¸",
    "ethereum": "ä»¥å¤ªåŠ",
    "stablecoin": "ç¨³å®šå¸",
    "central bank digital currency": "å¤®è¡Œæ•°å­—è´§å¸",
    "CBDC": "å¤®è¡Œæ•°å­—è´§å¸",
    "digital currency": "æ•°å­—è´§å¸",
    "digital asset": "æ•°å­—èµ„äº§",
    "blockchain": "åŒºå—é“¾",
    "decentralized finance": "å»ä¸­å¿ƒåŒ–é‡‘è",
    "DeFi": "å»ä¸­å¿ƒåŒ–é‡‘è",
    "token": "ä»£å¸",
    "virtual asset": "è™šæ‹Ÿèµ„äº§",
    "virtual currency": "è™šæ‹Ÿè´§å¸",
    "NFT": "éåŒè´¨åŒ–ä»£å¸",
    "mining": "æŒ–çŸ¿",
    "exchange": "äº¤æ˜“æ‰€",
    "wallet": "é’±åŒ…",
    "custody": "æ‰˜ç®¡",
    "anti-money laundering": "åæ´—é’±",
    "AML": "åæ´—é’±",
    "know your customer": "äº†è§£ä½ çš„å®¢æˆ·",
    "KYC": "äº†è§£ä½ çš„å®¢æˆ·",
    "fintech": "é‡‘èç§‘æŠ€",
    "payment": "æ”¯ä»˜",
    "settlement": "ç»“ç®—",
    "clearing": "æ¸…ç®—",
    "regulation": "ç›‘ç®¡",
    "compliance": "åˆè§„",
    "enforcement": "æ‰§æ³•",
    "sanctions": "åˆ¶è£",
    "risk": "é£é™©",
    "financial stability": "é‡‘èç¨³å®š",
    "monetary policy": "è´§å¸æ”¿ç­–",
    "interest rate": "åˆ©ç‡",
    "inflation": "é€šèƒ€",
    "liquidity": "æµåŠ¨æ€§",
    "capital": "èµ„æœ¬",
    "asset": "èµ„äº§",
    "securities": "è¯åˆ¸",
    "derivatives": "è¡ç”Ÿå“",
    "futures": "æœŸè´§",
    "options": "æœŸæƒ",
    "trading": "äº¤æ˜“",
    "market": "å¸‚åœº",
    "investor": "æŠ•èµ„è€…",
    "consumer protection": "æ¶ˆè´¹è€…ä¿æŠ¤",
    "disclosure": "æŠ«éœ²",
    "transparency": "é€æ˜åº¦",
    "framework": "æ¡†æ¶",
    "guidance": "æŒ‡å¼•",
    "consultation": "å’¨è¯¢",
    "proposal": "ææ¡ˆ",
    "rule": "è§„åˆ™",
    "press release": "æ–°é—»ç¨¿",
    "statement": "å£°æ˜",
    "speech": "è®²è¯",
    "report": "æŠ¥å‘Š",
    "research": "ç ”ç©¶",
    "analysis": "åˆ†æ",
    "review": "å®¡æŸ¥",
    "assessment": "è¯„ä¼°",
}


def translate_text(text: str) -> str:
    """
    ç®€æ˜“ç¿»è¯‘ï¼šåŸºäºè¯å…¸æ›¿æ¢å¸¸è§æœ¯è¯­
    å¯¹äºå®Œæ•´ç¿»è¯‘ï¼Œå»ºè®®æ¥å…¥ç¿»è¯‘ APIï¼ˆå¦‚ Google Translateã€DeepLï¼‰
    """
    if not text:
        return ""

    result = text
    # æŒ‰é•¿åº¦é™åºæ’åˆ—ï¼Œä¼˜å…ˆåŒ¹é…é•¿è¯ç»„
    sorted_terms = sorted(TRANSLATION_DICT.keys(), key=len, reverse=True)

    for en_term in sorted_terms:
        zh_term = TRANSLATION_DICT[en_term]
        # ä¸åŒºåˆ†å¤§å°å†™æ›¿æ¢
        pattern = re.compile(re.escape(en_term), re.IGNORECASE)
        result = pattern.sub(f"{zh_term}({en_term})", result)

    return result


def translate_to_chinese(text: str) -> str:
    """
    å°†è‹±æ–‡æ–‡æœ¬ç¿»è¯‘æˆä¸­æ–‡
    ä½¿ç”¨ translators åº“ï¼ˆæ”¯æŒå¤šä¸ªç¿»è¯‘å¼•æ“è‡ªåŠ¨åˆ‡æ¢ï¼‰
    """
    if not text or not text.strip():
        return ""

    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ä¸­æ–‡ä¸ºä¸»çš„å†…å®¹
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
    if chinese_chars > len(text) * 0.3:  # è¶…è¿‡30%æ˜¯ä¸­æ–‡ï¼Œä¸ç¿»è¯‘
        return text

    # æˆªæ–­è¿‡é•¿æ–‡æœ¬
    if len(text) > 500:
        text = text[:500]

    # å°è¯•å¤šä¸ªç¿»è¯‘å¼•æ“
    engines = ['bing', 'google', 'alibaba', 'baidu']

    for engine in engines:
        try:
            translated = ts.translate_text(
                text,
                translator=engine,
                from_language='en',
                to_language='zh'
            )
            if translated and translated != text:
                return translated
        except Exception as e:
            logger.debug(f"{engine} ç¿»è¯‘å¤±è´¥: {e}")
            continue

    # æ‰€æœ‰å¼•æ“éƒ½å¤±è´¥ï¼Œä½¿ç”¨è¯å…¸ç¿»è¯‘
    logger.warning(f"æ‰€æœ‰ç¿»è¯‘å¼•æ“å¤±è´¥ï¼Œä½¿ç”¨è¯å…¸ç¿»è¯‘")
    return translate_text(text)


def translate_with_api(text: str, target_lang: str = "zh") -> str:
    """
    ç¿»è¯‘æ–‡æœ¬åˆ°ä¸­æ–‡ï¼ˆå…¼å®¹æ—§æ¥å£ï¼‰
    """
    return translate_to_chinese(text)


# ============== é‡è¯•è£…é¥°å™¨ ==============

def retry_with_backoff(
    max_attempts: int = 3,
    backoff_base: int = 2,
    exceptions: tuple = (requests.RequestException,),
) -> Callable:
    """æŒ‡æ•°é€€é¿é‡è¯•è£…é¥°å™¨"""
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            last_exception = None
            for attempt in range(1, max_attempts + 1):
                try:
                    return func(*args, **kwargs)
                except exceptions as e:
                    last_exception = e
                    if attempt < max_attempts:
                        wait_time = backoff_base ** attempt
                        logger.warning(
                            f"{func.__name__} å¤±è´¥ (å°è¯• {attempt}/{max_attempts}): {e}"
                            f"ï¼Œ{wait_time}ç§’åé‡è¯•..."
                        )
                        time.sleep(wait_time)
                    else:
                        logger.error(
                            f"{func.__name__} æœ€ç»ˆå¤±è´¥ (å°è¯• {attempt}/{max_attempts}): {e}"
                        )
            raise last_exception
        return wrapper
    return decorator


# ============== é…ç½®åŠ è½½ ==============

def load_feeds() -> list[FeedSource]:
    """åŠ è½½ RSS æºé…ç½®"""
    if not FEEDS_FILE.exists():
        logger.error(f"RSS æºé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {FEEDS_FILE}")
        return []

    with open(FEEDS_FILE, "r", encoding="utf-8") as f:
        data = json.load(f)

    feeds = []
    for item in data.get("feeds", []):
        if item.get("enabled", True):
            feeds.append(FeedSource(
                name=item["name"],
                full_name=item.get("full_name", item["name"]),
                url=item["url"],
                tags=item.get("tags", []),
                enabled=item.get("enabled", True),
            ))

    logger.info(f"å·²åŠ è½½ {len(feeds)} ä¸ª RSS æº")
    return feeds


def load_config() -> AppConfig:
    """åŠ è½½åº”ç”¨é…ç½®ï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–"""
    config = AppConfig()

    if CONFIG_FILE.exists():
        with open(CONFIG_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)

        keywords = data.get("keywords", {})
        config.keywords_allow = keywords.get("allow", [])
        config.keywords_deny = keywords.get("deny", [])

        settings = data.get("settings", {})
        config.http_timeout = settings.get("http_timeout_seconds", 30)
        config.max_entries_per_feed = settings.get("max_entries_per_feed", 50)
        config.state_retention_days = settings.get("state_retention_days", 30)
        config.max_retries = settings.get("max_retries", 3)
        config.retry_backoff_base = settings.get("retry_backoff_base", 2)
        config.message_batch_size = settings.get("message_batch_size", 5)
        config.message_delay = settings.get("message_delay_seconds", 1.0)
        config.summary_max_length = settings.get("summary_max_length", 200)

        tags_filter = data.get("tags_filter", {})
        config.tags_filter_enabled = tags_filter.get("enabled", False)
        config.tags_include = tags_filter.get("include_tags", [])
        config.tags_exclude = tags_filter.get("exclude_tags", [])

    # ç¯å¢ƒå˜é‡è¦†ç›–
    if os.getenv("HTTP_TIMEOUT"):
        config.http_timeout = int(os.getenv("HTTP_TIMEOUT"))
    if os.getenv("MAX_ENTRIES_PER_FEED"):
        config.max_entries_per_feed = int(os.getenv("MAX_ENTRIES_PER_FEED"))
    if os.getenv("STATE_RETENTION_DAYS"):
        config.state_retention_days = int(os.getenv("STATE_RETENTION_DAYS"))

    logger.info(f"é…ç½®å·²åŠ è½½: å…è®¸å…³é”®è¯ {len(config.keywords_allow)} ä¸ªï¼Œ"
                f"æ‹’ç»å…³é”®è¯ {len(config.keywords_deny)} ä¸ª")
    return config


# ============== çŠ¶æ€ç®¡ç† ==============

def load_state() -> dict[str, Any]:
    """åŠ è½½çŠ¶æ€æ–‡ä»¶"""
    if not STATE_FILE.exists():
        return {"sent_ids": {}, "last_run": None}

    with open(STATE_FILE, "r", encoding="utf-8") as f:
        return json.load(f)


def save_state(state: dict[str, Any]) -> None:
    """ä¿å­˜çŠ¶æ€æ–‡ä»¶"""
    state["last_run"] = datetime.now(timezone.utc).isoformat()
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, ensure_ascii=False, indent=2)
    logger.info("çŠ¶æ€å·²ä¿å­˜")


def cleanup_state(state: dict[str, Any], retention_days: int) -> dict[str, Any]:
    """æ¸…ç†è¿‡æœŸçš„çŠ¶æ€è®°å½•"""
    cutoff = datetime.now(timezone.utc) - timedelta(days=retention_days)
    cutoff_str = cutoff.isoformat()

    sent_ids = state.get("sent_ids", {})
    original_count = len(sent_ids)

    # ä¿ç•™æœªè¿‡æœŸçš„è®°å½•
    cleaned = {
        entry_id: timestamp
        for entry_id, timestamp in sent_ids.items()
        if timestamp > cutoff_str
    }

    removed_count = original_count - len(cleaned)
    if removed_count > 0:
        logger.info(f"å·²æ¸…ç† {removed_count} æ¡è¿‡æœŸçŠ¶æ€è®°å½•")

    state["sent_ids"] = cleaned
    return state


def generate_entry_id(entry: dict, source_name: str) -> str:
    """ç”Ÿæˆæ¡ç›®å”¯ä¸€ ID"""
    # ä¼˜å…ˆä½¿ç”¨ entry è‡ªå¸¦çš„ id
    if entry.get("id"):
        return f"{source_name}:{entry['id']}"

    # å¦åˆ™ä½¿ç”¨ link çš„ hash
    if entry.get("link"):
        link_hash = hashlib.md5(entry["link"].encode()).hexdigest()[:12]
        return f"{source_name}:{link_hash}"

    # æœ€åä½¿ç”¨æ ‡é¢˜çš„ hash
    title_hash = hashlib.md5(entry.get("title", "").encode()).hexdigest()[:12]
    return f"{source_name}:{title_hash}"


# ============== RSS æŠ“å– ==============

@retry_with_backoff(max_attempts=3, backoff_base=2)
def fetch_feed(url: str, timeout: int = 30) -> feedparser.FeedParserDict:
    """æŠ“å– RSS æº"""
    # ä½¿ç”¨ requests è·å–å†…å®¹ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ§åˆ¶è¶…æ—¶å’Œé‡è¯•
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; CryptoPolicyBot/1.0)",
        "Accept": "application/rss+xml, application/xml, text/xml, */*",
    }

    response = requests.get(url, headers=headers, timeout=timeout)
    response.raise_for_status()

    return feedparser.parse(response.content)


def parse_entry_date(entry: dict) -> datetime:
    """è§£ææ¡ç›®å‘å¸ƒæ—¶é—´ï¼Œç¡®ä¿è¿”å› timezone-aware datetime"""
    # å°è¯•å¤šä¸ªæ—¥æœŸå­—æ®µ
    for date_field in ["published", "updated", "created"]:
        if entry.get(date_field):
            try:
                dt = date_parser.parse(entry[date_field])
                # ç¡®ä¿æ—¶åŒºæ„ŸçŸ¥
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                return dt
            except (ValueError, TypeError):
                continue

    # é»˜è®¤è¿”å›å½“å‰æ—¶é—´
    return datetime.now(timezone.utc)


def extract_summary(entry: dict, max_length: int = 200) -> str:
    """æå–æ‘˜è¦"""
    summary = ""

    # å°è¯•å¤šä¸ªæ‘˜è¦å­—æ®µ
    if entry.get("summary"):
        summary = entry["summary"]
    elif entry.get("description"):
        summary = entry["description"]
    elif entry.get("content"):
        contents = entry["content"]
        if isinstance(contents, list) and contents:
            summary = contents[0].get("value", "")

    # æ¸…ç† HTML æ ‡ç­¾
    summary = re.sub(r"<[^>]+>", "", summary)
    summary = re.sub(r"\s+", " ", summary).strip()

    # æˆªæ–­
    if len(summary) > max_length:
        summary = summary[:max_length].rsplit(" ", 1)[0] + "..."

    return summary


def matches_keywords(text: str, keywords: list[str]) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ¹é…å…³é”®è¯"""
    text_lower = text.lower()
    return any(kw.lower() in text_lower for kw in keywords)


def filter_entry(
    entry: dict,
    config: AppConfig,
    source_tags: list[str],
) -> bool:
    """è¿‡æ»¤æ¡ç›®ï¼šè¿”å› True è¡¨ç¤ºä¿ç•™"""
    title = entry.get("title", "")
    summary = entry.get("summary", "") or entry.get("description", "")
    combined_text = f"{title} {summary}"

    # æ£€æŸ¥æ‹’ç»å…³é”®è¯
    if config.keywords_deny and matches_keywords(combined_text, config.keywords_deny):
        return False

    # æ£€æŸ¥å…è®¸å…³é”®è¯ï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
    if config.keywords_allow:
        if not matches_keywords(combined_text, config.keywords_allow):
            return False

    # æ£€æŸ¥æ ‡ç­¾è¿‡æ»¤
    if config.tags_filter_enabled:
        if config.tags_exclude:
            if any(tag in config.tags_exclude for tag in source_tags):
                return False
        if config.tags_include:
            if not any(tag in config.tags_include for tag in source_tags):
                return False

    return True


def process_feed(
    source: FeedSource,
    config: AppConfig,
    sent_ids: set[str],
) -> list[FeedEntry]:
    """å¤„ç†å•ä¸ª RSS æº"""
    logger.info(f"æ­£åœ¨æŠ“å–: {source.name} ({source.url})")

    try:
        feed = fetch_feed(source.url, config.http_timeout)
    except Exception as e:
        logger.error(f"æŠ“å– {source.name} å¤±è´¥: {e}")
        return []

    entries = []
    for raw_entry in feed.entries[:config.max_entries_per_feed]:
        entry_id = generate_entry_id(raw_entry, source.name)

        # è·³è¿‡å·²å‘é€
        if entry_id in sent_ids:
            continue

        # è¿‡æ»¤
        if not filter_entry(raw_entry, config, source.tags):
            continue

        # æå–ä¿¡æ¯
        title = raw_entry.get("title", "æ— æ ‡é¢˜")
        summary = extract_summary(raw_entry, config.summary_max_length)

        # ç¿»è¯‘ï¼ˆæ·»åŠ å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶ï¼‰
        title_zh = translate_to_chinese(title)
        time.sleep(1.0)  # é¿å…ç¿»è¯‘APIé€Ÿç‡é™åˆ¶
        summary_zh = translate_to_chinese(summary)
        time.sleep(1.0)

        entry = FeedEntry(
            id=entry_id,
            title=title,
            title_zh=title_zh,
            link=raw_entry.get("link", ""),
            summary=summary,
            summary_zh=summary_zh,
            published=parse_entry_date(raw_entry),
            source=source.name,
            source_full=source.full_name,
            tags=source.tags,
        )
        entries.append(entry)

    logger.info(f"{source.name}: å‘ç° {len(entries)} æ¡æ–°æ¡ç›®")
    return entries


# ============== ä¼ä¸šå¾®ä¿¡å‘é€ ==============

def format_wecom_markdown(entries: list[FeedEntry]) -> str:
    """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡ Markdown æ¶ˆæ¯ï¼ˆä¸­æ–‡ç‰ˆï¼‰"""
    # åŒ—äº¬æ—¶é—´
    beijing_tz = timezone(timedelta(hours=8))
    now_beijing = datetime.now(beijing_tz)

    lines = [
        "# ğŸ“š åŠ å¯†æ”¿ç­–/ç ”æŠ¥é€Ÿè§ˆ",
        f"> â° {now_beijing.strftime('%Y-%m-%d %H:%M')} åŒ—äº¬æ—¶é—´",
        "",
    ]

    for i, entry in enumerate(entries, 1):
        # æ¥æºæ ‡ç­¾ï¼ˆç»¿è‰²é«˜äº®ï¼‰
        source_tag = f"<font color=\"info\">[{entry.source}]</font>"

        # æ ‡é¢˜ï¼ˆä¸­æ–‡ä¼˜å…ˆï¼‰
        lines.append(f"**{i}. {source_tag} {entry.title_zh}**")

        # æ‘˜è¦ï¼ˆä¸­æ–‡ï¼‰
        if entry.summary_zh:
            summary_text = entry.summary_zh[:150]
            if len(entry.summary_zh) > 150:
                summary_text += "..."
            lines.append(f"> {summary_text}")

        # é“¾æ¥
        lines.append(f"[ğŸ‘‰ é˜…è¯»åŸæ–‡]({entry.link})")
        lines.append("")

    # æ ‡ç­¾
    all_tags = set()
    for entry in entries:
        all_tags.update(entry.tags)
    if all_tags:
        tag_str = " ".join(f"`#{tag}`" for tag in sorted(all_tags)[:5])
        lines.append(tag_str)

    return "\n".join(lines)


def format_wecom_text(entries: list[FeedEntry]) -> str:
    """æ ¼å¼åŒ–ä¼ä¸šå¾®ä¿¡çº¯æ–‡æœ¬æ¶ˆæ¯ï¼ˆå¤‡ç”¨ï¼‰"""
    beijing_tz = timezone(timedelta(hours=8))
    now_beijing = datetime.now(beijing_tz)

    lines = [
        "ğŸ“š åŠ å¯†æ”¿ç­–/ç ”æŠ¥é€Ÿè§ˆ",
        f"â° {now_beijing.strftime('%Y-%m-%d %H:%M')} åŒ—äº¬æ—¶é—´",
        "â”" * 20,
        "",
    ]

    for i, entry in enumerate(entries, 1):
        lines.append(f"{i}. [{entry.source}] {entry.title_zh}")
        if entry.summary_zh:
            lines.append(f"   ğŸ“ {entry.summary_zh[:120]}...")
        lines.append(f"   ğŸ”— {entry.link}")
        lines.append("")

    return "\n".join(lines)


@retry_with_backoff(max_attempts=3, backoff_base=2)
def send_wecom_message(content: str, webhook_url: str, msg_type: str = "markdown") -> bool:
    """
    å‘é€ä¼ä¸šå¾®ä¿¡æ¶ˆæ¯

    Args:
        content: æ¶ˆæ¯å†…å®¹
        webhook_url: ä¼ä¸šå¾®ä¿¡ Webhook URL
        msg_type: æ¶ˆæ¯ç±»å‹ (markdown / text)
    """
    if msg_type == "markdown":
        payload = {
            "msgtype": "markdown",
            "markdown": {
                "content": content
            }
        }
    else:
        payload = {
            "msgtype": "text",
            "text": {
                "content": content
            }
        }

    response = requests.post(webhook_url, json=payload, timeout=30)
    response.raise_for_status()

    result = response.json()
    if result.get("errcode") != 0:
        raise Exception(f"ä¼ä¸šå¾®ä¿¡ API é”™è¯¯: {result}")

    logger.debug(f"ä¼ä¸šå¾®ä¿¡å“åº”: {result}")
    return True


def send_entries(
    entries: list[FeedEntry],
    batch_size: int = 5,
    delay: float = 1.0,
) -> list[str]:
    """æ‰¹é‡å‘é€æ¡ç›®åˆ°ä¼ä¸šå¾®ä¿¡"""
    if not entries:
        logger.info("æ²¡æœ‰æ–°æ¡ç›®éœ€è¦å‘é€")
        return []

    if DRY_RUN:
        logger.info(f"[DRY-RUN] å°†å‘é€ {len(entries)} æ¡æ¶ˆæ¯")
        for entry in entries:
            logger.info(f"  - [{entry.source}] {entry.title_zh}")
        # æ‰“å°æ¶ˆæ¯é¢„è§ˆ
        preview = format_wecom_markdown(entries[:3])
        logger.info(f"\næ¶ˆæ¯é¢„è§ˆ:\n{preview}")
        return [e.id for e in entries]

    if not WECOM_WEBHOOK_URL:
        logger.error("ä¼ä¸šå¾®ä¿¡é…ç½®ç¼ºå¤±: è¯·è®¾ç½® WECOM_WEBHOOK_URL ç¯å¢ƒå˜é‡")
        return []

    sent_ids = []

    # åˆ†æ‰¹å‘é€ï¼ˆä¼ä¸šå¾®ä¿¡å•æ¡æ¶ˆæ¯é™åˆ¶ 4096 å­—èŠ‚ï¼‰
    for i in range(0, len(entries), batch_size):
        batch = entries[i:i + batch_size]
        message = format_wecom_markdown(batch)

        # å¦‚æœæ¶ˆæ¯è¿‡é•¿ï¼Œå°è¯•ä½¿ç”¨çº¯æ–‡æœ¬æ ¼å¼
        if len(message.encode('utf-8')) > 4000:
            logger.warning("æ¶ˆæ¯è¿‡é•¿ï¼Œåˆ‡æ¢ä¸ºçº¯æ–‡æœ¬æ ¼å¼")
            message = format_wecom_text(batch)

        try:
            send_wecom_message(message, WECOM_WEBHOOK_URL, "markdown")
            sent_ids.extend(e.id for e in batch)
            logger.info(f"å·²å‘é€ç¬¬ {i // batch_size + 1} æ‰¹ ({len(batch)} æ¡)")

            # æ‰¹æ¬¡é—´å»¶è¿Ÿï¼ˆä¼ä¸šå¾®ä¿¡é™åˆ¶æ¯åˆ†é’Ÿ 20 æ¡ï¼‰
            if i + batch_size < len(entries):
                time.sleep(delay)

        except Exception as e:
            logger.error(f"å‘é€å¤±è´¥: {e}")
            # ç»§ç»­å°è¯•ä¸‹ä¸€æ‰¹

    return sent_ids


# ============== ä¸»å‡½æ•° ==============

def main() -> int:
    """ä¸»å‡½æ•°"""
    logger.info("=" * 50)
    logger.info("åŠ å¯†æ”¿ç­– RSS èšåˆå™¨å¯åŠ¨")
    logger.info(f"DRY_RUN: {DRY_RUN}")
    logger.info("=" * 50)

    # åŠ è½½é…ç½®
    feeds = load_feeds()
    if not feeds:
        logger.error("æ²¡æœ‰å¯ç”¨çš„ RSS æº")
        return 1

    config = load_config()

    # åŠ è½½å¹¶æ¸…ç†çŠ¶æ€
    state = load_state()
    state = cleanup_state(state, config.state_retention_days)
    sent_ids = set(state.get("sent_ids", {}).keys())

    # å¤„ç†æ‰€æœ‰ RSS æº
    all_entries: list[FeedEntry] = []

    for source in feeds:
        try:
            entries = process_feed(source, config, sent_ids)
            all_entries.extend(entries)
        except Exception as e:
            logger.error(f"å¤„ç† {source.name} æ—¶å‡ºé”™: {e}")
            # å•ä¸ªæºå¤±è´¥ä¸å½±å“å…¶ä»–æº
            continue

    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    all_entries.sort(key=lambda e: e.published, reverse=True)

    logger.info(f"å…±å‘ç° {len(all_entries)} æ¡æ–°æ¡ç›®")

    # å‘é€
    newly_sent = send_entries(
        all_entries,
        batch_size=config.message_batch_size,
        delay=config.message_delay,
    )

    # æ›´æ–°çŠ¶æ€
    now_iso = datetime.now(timezone.utc).isoformat()
    for entry_id in newly_sent:
        state["sent_ids"][entry_id] = now_iso

    save_state(state)

    logger.info(f"æœ¬æ¬¡è¿è¡Œå®Œæˆ: å‘é€ {len(newly_sent)} æ¡")
    logger.info("=" * 50)

    return 0


if __name__ == "__main__":
    sys.exit(main())
